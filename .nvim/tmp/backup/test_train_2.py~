import pytest
import pickle

from nltk.tokenize.punkt import PunktSentenceTokenizer
from nltk import word_tokenize

from ocr.train import Train
from ocr.features import ServiceKenLM, ServiceNgrams

import sys

@pytest.fixture(scope='session')
def sents():
    return open('./ocr/test/fixtures/vv-22-06-1965_small.txt', 'r')

@pytest.fixture(scope='session')
def service():
    with open('./ocr/test/fixtures/samanthas-droom.txt', 'r') as f:
        raw = f.read()
        sents = PunktSentenceTokenizer().sentences_from_text(raw)
        tokened_sents = [word_tokenize(s) for s in sents]
    return ServiceKenLM()

def test_assert_sents():
    clean = ['<s>','Zij', 'ging', ('vroeg',1), 'naar bed', '.']
    best =  ['<s>','Zij', 'ging', '__null__', ('vroeg',1), 'naar bed', '.']
    t = Train(service=service)
    assert t.assert_sents(best, clean)

def test_train_1(sents,service):
    print('******',sys.path)
    dirty = []
    clean = []
    for i, line in enumerate(sents):
        if (i-0) % 3 == 0:
            dirty.append([w.strip("\n").strip("'") for w in line.split(", ")])
        elif (i-1) % 3 == 0:
            clean.append([w.strip("\n").strip("'") for w in line.split(", ")])

    #t = Train(service=service)
    assert False
    #assert t.train(dirty, clean) == (
           #0.027098388388812823, 0.20413926851582248, 0.1, 0.1 )
